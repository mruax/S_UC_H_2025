# pip install natasha

from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab
from collections import Counter, defaultdict
import re
import json
from typing import Dict, List, Set, Tuple, Optional
from pathlib import Path

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha ---
segmenter = Segmenter()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
morph_vocab = MorphVocab()

# --- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ ---
STOPWORDS = {
    "–∫—É—Ä—Å", "–¥–ª—è", "–∏–∑—É—á–µ–Ω–∏–µ", "–æ—Å–Ω–æ–≤–∞", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–ø–æ", "—É—Ä–æ–∫", "–∑–∞–Ω—è—Ç–∏–µ",
    "–æ–±—É—á–µ–Ω–∏–µ", "–≥–æ–¥", "—ç—Ç–æ", "–±—ã—Ç—å", "–∫–æ—Ç–æ—Ä—ã–π", "–º–æ—á—å", "–≤–µ—Å—å", "—Å–≤–æ–π", "–Ω–∞—à",
    "–≤–∞—à", "—Ç–æ—Ç", "—ç—Ç–æ—Ç", "–º–æ–π", "—Ç–≤–æ–π", "–µ–≥–æ", "–µ—ë", "–Ω–∞—à", "—É—á–∏—Ç—å", "–Ω–∞—É—á–∏—Ç—å—Å—è",
    "–ø–æ–ª—É—á–∏—Ç—å", "—É–∑–Ω–∞—Ç—å", "–ø–æ–Ω—è—Ç—å", "–æ—Å–≤–æ–∏—Ç—å", "–æ–¥–∏–Ω", "–¥–≤–∞", "—Ç—Ä–∏"
}

# –û—Ç–≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã
PROCESS_WORDS = {
    "–∏–∑—É—á–µ–Ω–∏–µ", "—Å–æ–∑–¥–∞–Ω–∏–µ", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ",
    "–æ—Å–≤–æ–µ–Ω–∏–µ", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ", "–ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ", "—Ä–∞–±–æ—Ç–∞", "–æ–±—É—á–µ–Ω–∏–µ",
    "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è", "–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ", "—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
}

# –û–±—â–∏–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∏ —Å–ª–æ–≤–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
GENERIC_ADJECTIVES = {
    "–æ—Å–Ω–æ–≤–Ω–æ–π", "–≥–ª–∞–≤–Ω—ã–π", "–≤–∞–∂–Ω—ã–π", "–Ω–æ–≤—ã–π", "—Å—Ç–∞—Ä—ã–π", "–±–æ–ª—å—à–æ–π", "–º–∞–ª—ã–π",
    "—Ö–æ—Ä–æ—à–∏–π", "–ø–ª–æ—Ö–æ–π", "–ø—Ä–æ—Å—Ç–æ–π", "—Å–ª–æ–∂–Ω—ã–π", "–ø–æ–ª–Ω—ã–π", "—á–∞—Å—Ç–∏—á–Ω—ã–π", "–æ–±—â–∏–π",
    "—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π", "—Ä–∞–∑–ª–∏—á–Ω—ã–π", "—Ä–∞–∑–Ω—ã–π", "–ø–µ—Ä–≤—ã–π", "–ø–æ—Å–ª–µ–¥–Ω–∏–π", "—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π",
    "–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π", "—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π", "–≥–ª—É–±–æ–∫–∏–π", "–ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–π", "–Ω–∞—á–∏–Ω–∞—é—â–∏–π",
    "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π", "–±–∞–∑–æ–≤—ã–π", "—Å—Ä–µ–¥–Ω–∏–π", "–Ω–∞—á–∞–ª—å–Ω—ã–π", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π"
}

# --- –ò–µ—Ä–∞—Ä—Ö–∏—è –æ–±–ª–∞—Å—Ç–µ–π ---
AREAS = {
    "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": {
        "keywords": ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–∫–æ–¥", "coding", "programming", "python", "java", "javascript",
                     "c++", "–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"],
        "priority": 10,
        "related_words": ["—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "developer", "—Å–∞–π—Ç", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"],
        "default_categories": ["–∞–ª–≥–æ—Ä–∏—Ç–º—ã", "—Å–∏–Ω—Ç–∞–∫—Å–∏—Å —è–∑—ã–∫–∞"]
    },
    "–¥–∞–Ω–Ω—ã–µ": {
        "keywords": ["–¥–∞–Ω–Ω—ã–µ", "data", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "analytics", "–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", "big data", "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"],
        "priority": 9,
        "related_words": ["–∞–Ω–∞–ª–∏—Ç–∏–∫", "analyst", "–æ–±—Ä–∞–±–æ—Ç–∫–∞"],
        "default_categories": ["–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è"]
    },
    "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ": {
        "keywords": ["–º–∞—à–∏–Ω–Ω—ã–π", "–æ–±—É—á–µ–Ω–∏–µ", "ml", "machine learning", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "ai", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
                     "–Ω–µ–π—Ä–æ–Ω–Ω—ã–π"],
        "priority": 9,
        "related_words": ["–º–æ–¥–µ–ª—å", "–∞–ª–≥–æ—Ä–∏—Ç–º", "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"],
        "default_categories": ["–º–æ–¥–µ–ª–∏", "–∞–ª–≥–æ—Ä–∏—Ç–º—ã"]
    },
    "–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞": {
        "keywords": ["–≤–µ–±", "web", "—Å–∞–π—Ç", "frontend", "backend", "fullstack", "react", "node", "html", "css"],
        "priority": 8,
        "related_words": ["–±—Ä–∞—É–∑–µ—Ä", "http", "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç"],
        "default_categories": ["–≤–µ–±-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥"]
    },
    "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞": {
        "keywords": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è", "math", "–º–∞—Ç–∞–Ω", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
                     "–∞–Ω–∞–ª–∏–∑"],
        "priority": 7,
        "related_words": ["—Ñ–æ—Ä–º—É–ª–∞", "—É—Ä–∞–≤–Ω–µ–Ω–∏–µ", "—Ç–µ–æ—Ä–µ–º–∞", "—Ñ—É–Ω–∫—Ü–∏—è"],
        "default_categories": ["–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", "—Ç–µ–æ—Ä–∏—è"]
    },
    "–¥–∏–∑–∞–π–Ω": {
        "keywords": ["–¥–∏–∑–∞–π–Ω", "design", "–≥—Ä–∞—Ñ–∏–∫–∞", "ui", "ux", "figma", "photoshop", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"],
        "priority": 7,
        "related_words": ["–º–∞–∫–µ—Ç", "–ø—Ä–æ—Ç–æ—Ç–∏–ø", "–∫–æ–º–ø–æ–∑–∏—Ü–∏—è"],
        "default_categories": ["UI/UX –¥–∏–∑–∞–π–Ω", "–≤–∏–∑—É–∞–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω"]
    },
    "–±–∏–∑–Ω–µ—Å": {
        "keywords": ["–±–∏–∑–Ω–µ—Å", "business", "–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–ø—Ä–æ–¥–∞–∂–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
        "priority": 6,
        "related_words": ["—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–ø—Ä–æ–¥–∞–∂–∏", "—Ñ–∏–Ω–∞–Ω—Å—ã", "–±—é–¥–∂–µ—Ç"],
        "default_categories": ["–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è"]
    },
    "—è–∑—ã–∫–∏": {
        "keywords": ["–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "–Ω–µ–º–µ—Ü–∫–∏–π", "–∏—Å–ø–∞–Ω—Å–∫–∏–π", "–∫–∏—Ç–∞–π—Å–∫–∏–π", "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π", "—è–∑—ã–∫", "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞",
                     "–ª–µ–∫—Å–∏–∫–∞"],
        "priority": 8,
        "related_words": ["–ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ", "–¥–∏–∞–ª–æ–≥", "—Ä–∞–∑–≥–æ–≤–æ—Ä", "–∏–µ—Ä–æ–≥–ª–∏—Ñ"],
        "default_categories": ["–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞", "–ª–µ–∫—Å–∏–∫–∞"]
    },
    "–ª–∏—á–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ": {
        "keywords": ["–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—Ç–∞–π–º-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç", "–º–æ—Ç–∏–≤–∞—Ü–∏—è", "—Ä–∞–∑–≤–∏—Ç–∏–µ", "–Ω–∞–≤—ã–∫", "–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è"],
        "priority": 5,
        "related_words": ["—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—Ü–µ–ª—å", "–ø—Ä–∏–≤—ã—á–∫–∞"],
        "default_categories": ["soft skills", "—Å–∞–º–æ–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è"]
    }
}

# --- –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã ---
TECHNOLOGIES = {
    "python": ["python", "–ø–∏—Ç–æ–Ω"],
    "java": ["java", "–¥–∂–∞–≤–∞"],
    "javascript": ["javascript", "js", "typescript", "node"],
    "c++": ["c++", "cpp", "—Å–∏++"],
    "sql": ["sql", "–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "database", "postgresql", "mysql"],
    "pandas": ["pandas"],
    "numpy": ["numpy"],
    "tensorflow": ["tensorflow", "tf"],
    "pytorch": ["pytorch"],
    "react": ["react", "—Ä–µ–∞–∫—Ç"],
    "django": ["django", "–¥–∂–∞–Ω–≥–æ"],
    "flask": ["flask"],
    "docker": ["docker", "–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä"],
    "git": ["git", "github", "gitlab"],
    "html/css": ["html", "css", "–≤–µ—Ä—Å—Ç–∫–∞"],
    "figma": ["figma"],
    "excel": ["excel", "—Ç–∞–±–ª–∏—Ü–∞"],
}

# --- –ü—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ ---
DOMAIN_CONCEPTS = {
    "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏": ["–Ω–µ–π—Ä–æ–Ω–Ω—ã–π —Å–µ—Ç—å", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "neural network"],
    "API": ["api", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"],
    "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö": ["–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "database", "–±–¥"],
    "–∞–ª–≥–æ—Ä–∏—Ç–º—ã": ["–∞–ª–≥–æ—Ä–∏—Ç–º"],
    "—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö": ["—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"],
    "–≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è": ["–≤–µ–± –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "web application"],
    "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã": ["–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "ui", "gui"],
    "–ø—Ä–æ—Ç–æ—Ç–∏–ø—ã": ["–ø—Ä–æ—Ç–æ—Ç–∏–ø", "prototype"],
    "–º–∞–∫–µ—Ç—ã": ["–º–∞–∫–µ—Ç", "layout"],
    "–º–æ–¥–µ–ª–∏": ["–º–æ–¥–µ–ª—å", "model"],
    "–ø—Ä–µ–¥–µ–ª—ã": ["–ø—Ä–µ–¥–µ–ª", "limit"],
    "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ": ["–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–π", "derivative"],
    "–∏–Ω—Ç–µ–≥—Ä–∞–ª—ã": ["–∏–Ω—Ç–µ–≥—Ä–∞–ª", "integral"],
    "–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã": ["–∫–æ–º–ø–æ–Ω–µ–Ω—Ç", "component"],
    "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è": ["—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "–∑–∞–¥–∞—á–∞", "–∑–∞–¥–∞–Ω–∏–µ"],
    "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞": ["–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞", "grammar"],
    "–ª–µ–∫—Å–∏–∫–∞": ["–ª–µ–∫—Å–∏–∫–∞", "—Å–ª–æ–≤–∞—Ä—å", "vocabulary"],
    "–ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ": ["–ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏–µ", "—Ñ–æ–Ω–µ—Ç–∏–∫–∞"],
    "—Ñ–∏–Ω–∞–Ω—Å—ã": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "–¥–µ–Ω—å–≥–∏", "–±—é–¥–∂–µ—Ç", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è"],
    "–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è": ["–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è", "—É—á–µ—Ç", "–±–∞–ª–∞–Ω—Å"],
}

# --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ---
CATEGORIES = {
    "–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö": ["–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö", "data analysis", "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è", "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "pandas", "numpy"],
    "–∞–ª–≥–æ—Ä–∏—Ç–º—ã": ["–∞–ª–≥–æ—Ä–∏—Ç–º", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö", "—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞", "–≥—Ä–∞—Ñ", "–¥–µ—Ä–µ–≤–æ"],
    "–≤–µ–±-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": ["–≤–µ–±", "http", "api", "rest", "—Å–µ—Ä–≤–µ—Ä", "–∫–ª–∏–µ–Ω—Ç", "–≤–µ–± –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"],
    "–º–æ–±–∏–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞": ["–º–æ–±–∏–ª—å–Ω—ã–π", "android", "ios", "mobile"],
    "devops": ["devops", "ci/cd", "deployment", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "docker", "kubernetes"],
    "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": ["—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "testing", "qa", "—Ç–µ—Å—Ç", "pytest"],
    "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": ["–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å", "security", "–∑–∞—â–∏—Ç–∞", "—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ"],
    "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö": ["–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "sql", "nosql", "–∑–∞–ø—Ä–æ—Å", "—Ç–∞–±–ª–∏—Ü–∞"],
    "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–ø–∞—Ç—Ç–µ—Ä–Ω", "design pattern"],
    "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏": ["–Ω–µ–π—Ä–æ–Ω–Ω—ã–π", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å", "deep learning", "–≥–ª—É–±–æ–∫–∏–π –æ–±—É—á–µ–Ω–∏–µ"],
    "–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑": ["–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", "–º–∞—Ç–∞–Ω", "–ø—Ä–µ–¥–µ–ª", "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–π", "–∏–Ω—Ç–µ–≥—Ä–∞–ª"],
    "UI/UX –¥–∏–∑–∞–π–Ω": ["ui", "ux", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç", "—é–∑–∞–±–∏–ª–∏—Ç–∏"],
    "–ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–ø—Ä–æ—Ç–æ—Ç–∏–ø", "–º–∞–∫–µ—Ç", "wireframe"],
    "—Å–∏–Ω—Ç–∞–∫—Å–∏—Å —è–∑—ã–∫–∞": ["—Å–∏–Ω—Ç–∞–∫—Å–∏—Å", "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞", "–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"],
    "–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è": ["–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è", "–≥—Ä–∞—Ñ–∏–∫", "–¥–∏–∞–≥—Ä–∞–º–º–∞", "plotting"],
    "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥": ["frontend", "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥", "–∫–ª–∏–µ–Ω—Ç—Å–∫–∏–π"],
    "–±—ç–∫–µ–Ω–¥": ["backend", "–±—ç–∫–µ–Ω–¥", "—Å–µ—Ä–≤–µ—Ä–Ω—ã–π"],
    "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞": ["–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞", "–≤—Ä–µ–º–µ–Ω–∞", "–ø–∞–¥–µ–∂", "—Å–∫–ª–æ–Ω–µ–Ω–∏–µ"],
    "–ª–µ–∫—Å–∏–∫–∞": ["–ª–µ–∫—Å–∏–∫–∞", "—Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å", "vocabulary"],
    "—Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞": ["—Ä–∞–∑–≥–æ–≤–æ—Ä", "–¥–∏–∞–ª–æ–≥", "–æ–±—â–µ–Ω–∏–µ", "—Ä–µ—á—å"],
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", "–±—é–¥–∂–µ—Ç", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è", "–ø–µ–Ω—Å–∏—è"],
    "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏": ["—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç", "pmi", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Ä–∏—Å–∫"],
    "soft skills": ["–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è", "–ª–∏–¥–µ—Ä—Å—Ç–≤–æ", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥–∞", "presentation"],
    "–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å": ["–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "–≤—Ä–µ–º—è", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"],
}

# --- –ê—Ç—Ä–∏–±—É—Ç—ã ---
ATTRIBUTES = {
    "–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π": ["–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π", "–ø—Ä–æ–µ–∫—Ç", "hands-on", "–ø—Ä–∞–∫—Ç–∏–∫–∞", "–∑–∞–¥–∞—á–∞", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ", "–∑–∞–¥–∞–Ω–∏–µ"],
    "–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–π": ["–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–π", "bootcamp", "–±—É—Ç–∫–µ–º–ø", "—É—Å–∫–æ—Ä–µ–Ω–Ω—ã–π", "–∏–Ω—Ç–µ–Ω—Å–∏–≤"],
    "–∫—Ä–∞—Ç–∫–∏–π": ["–∫—Ä–∞—Ç–∫–∏–π", "–º–∏–Ω–∏", "–±—ã—Å—Ç—Ä—ã–π", "short", "—ç–∫—Å–ø—Ä–µ—Å—Å", "–∫–æ–º–ø–∞–∫—Ç–Ω—ã–π"],
    "—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π": ["—Ç–µ–æ—Ä–∏—è", "–ª–µ–∫—Ü–∏—è", "–æ–±–∑–æ—Ä", "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π", "–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π"],
    "—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è": ["—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "certification", "–∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è", "–¥–∏–ø–ª–æ–º"],
    "–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π": ["–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π", "interactive", "–∂–∏–≤–æ–π", "–æ–Ω–ª–∞–π–Ω-–∑–∞–Ω—è—Ç–∏–µ"],
}

# --- –£—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ ---
DIFFICULTY = {
    "–Ω–∞—á–∞–ª—å–Ω—ã–π": ["–Ω–∞—á–∞–ª—å–Ω—ã–π", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–æ—Å–Ω–æ–≤—ã", "–Ω–∞—á–∏–Ω–∞—é—â–∏–π", "–±–∞–∑–æ–≤—ã–π", "beginner", "basic", "–Ω—É–ª—å", "—Å –Ω—É–ª—è"],
    "—Å—Ä–µ–¥–Ω–∏–π": ["—Å—Ä–µ–¥–Ω–∏–π", "intermediate", "–ø—Ä–æ–¥–æ–ª–∂–∞—é—â–∏–π", "—Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å"],
    "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π": ["–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π", "advanced", "—ç–∫—Å–ø–µ—Ä—Ç", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", "–≥–ª—É–±–æ–∫–∏–π", "—É–≥–ª—É–±–ª–µ–Ω–Ω—ã–π"]
}

# --- –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É ---
DIFFICULTY_INDICATORS = {
    "–Ω–∞—á–∞–ª—å–Ω—ã–π": ["–æ—Å–Ω–æ–≤–∞", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–Ω–∞—á–∞–ª–æ", "–ø–µ—Ä–≤—ã–π —à–∞–≥", "–∑–Ω–∞–∫–æ–º—Å—Ç–≤–æ", "–±–∞–∑–æ–≤—ã–π"],
    "—Å—Ä–µ–¥–Ω–∏–π": ["–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è", "—Å–æ–∑–¥–∞–Ω–∏–µ"],
    "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π": ["–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π"]
}


class CourseTagGenerator:
    def __init__(self):
        self.segmenter = segmenter
        self.morph_tagger = morph_tagger
        self.morph_vocab = morph_vocab

    def analyze_tokens(self, text: str) -> List[Tuple[str, str]]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏"""
        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)

        tokens_info = []
        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)
            lemma = token.lemma.lower()
            pos = token.pos
            if len(lemma) > 2 and lemma not in STOPWORDS and lemma.isalpha():
                tokens_info.append((lemma, pos))

        return tokens_info

    def extract_meaningful_phrases(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –û–°–ú–´–°–õ–ï–ù–ù–´–ï —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è"""
        doc = Doc(text)
        doc.segment(self.segmenter)
        doc.tag_morph(self.morph_tagger)

        for token in doc.tokens:
            token.lemmatize(self.morph_vocab)

        phrases = []
        tokens = [t for t in doc.tokens if len(t.lemma) > 2 and t.lemma.lower() not in STOPWORDS]

        for i in range(len(tokens) - 1):
            current = tokens[i]
            next_token = tokens[i + 1]

            current_lemma = current.lemma.lower()
            next_lemma = next_token.lemma.lower()

            if current_lemma in GENERIC_ADJECTIVES:
                continue

            if current_lemma in PROCESS_WORDS:
                continue

            if current.pos == 'NOUN' and next_token.pos == 'NOUN':
                if next_lemma not in PROCESS_WORDS and next_lemma not in GENERIC_ADJECTIVES:
                    phrase = f"{current_lemma} {next_lemma}"
                    phrases.append(phrase)

            elif current.pos == 'ADJ' and next_token.pos == 'NOUN':
                if next_lemma not in PROCESS_WORDS:
                    phrase = f"{current_lemma} {next_lemma}"
                    phrases.append(phrase)

        return phrases

    def normalize_text(self, text: str) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        tokens_info = self.analyze_tokens(text)

        normalized = []
        for lemma, pos in tokens_info:
            if lemma in PROCESS_WORDS or lemma in GENERIC_ADJECTIVES:
                continue

            if pos in ['NOUN', 'PROPN'] or any(lemma in techs for techs in TECHNOLOGIES.values()):
                normalized.append(lemma)

        return normalized

    def extract_phrases(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã"""
        words = re.findall(r'\w+', text.lower())
        phrases = []

        phrases.extend(words)

        for i in range(len(words) - 1):
            phrases.append(f"{words[i]} {words[i + 1]}")

        for i in range(len(words) - 2):
            phrases.append(f"{words[i]} {words[i + 1]} {words[i + 2]}")

        return phrases

    def score_match(self, text: str, keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∞–±–æ—Ä—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        text_lower = text.lower()
        phrases = self.extract_phrases(text)

        score = 0
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in text_lower.split():
                score += 2
            elif any(keyword_lower in phrase for phrase in phrases):
                score += 1

        return score

    def is_similar_or_contains(self, tag1: str, tag2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–∏ –ª–∏ —Ç–µ–≥–∏ –∏–ª–∏ –æ–¥–∏–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–º"""
        tag1_lower = tag1.lower()
        tag2_lower = tag2.lower()

        if tag1_lower in tag2_lower or tag2_lower in tag1_lower:
            return True

        words1 = set(tag1_lower.split())
        words2 = set(tag2_lower.split())

        if len(words1) > 0 and len(words2) > 0:
            intersection = words1.intersection(words2)
            min_len = min(len(words1), len(words2))
            if len(intersection) / min_len > 0.5:
                return True

        return False

    def determine_area(self, title: str, description: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∫—É—Ä—Å–∞"""
        full_text = f"{title} {description}"

        scores = {}
        for area, data in AREAS.items():
            match_score = self.score_match(full_text, data["keywords"])
            related_score = self.score_match(full_text, data.get("related_words", []))
            scores[area] = (match_score + related_score * 0.5) * data["priority"]

        if scores and max(scores.values()) > 0:
            return max(scores, key=scores.get)

        for tech, keywords in TECHNOLOGIES.items():
            if self.score_match(full_text, keywords) > 0:
                if tech in ["python", "java", "javascript", "c++"]:
                    return "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
                elif tech in ["tensorflow", "pytorch"]:
                    return "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
                elif tech in ["react", "html/css", "django", "flask"]:
                    return "–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"
                elif tech in ["sql", "pandas", "numpy"]:
                    return "–¥–∞–Ω–Ω—ã–µ"
                elif tech in ["figma"]:
                    return "–¥–∏–∑–∞–π–Ω"

        tech_indicators = ["–ø—Ä–æ–≥—Ä–∞–º–º", "–∫–æ–¥", "—Ä–∞–∑—Ä–∞–±–æ—Ç", "—Ç–µ—Ö–Ω", "it"]
        if any(ind in full_text.lower() for ind in tech_indicators):
            return "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"

        return "–æ–±—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ"

    def determine_thematic_tags(self, title: str, description: str, area: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ + –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏"""
        full_text = f"{title} {description}"

        tags = []
        excluded_tags = [area] if area else []

        # 1. –ò—â–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        tech_scores = {}
        for tech, keywords in TECHNOLOGIES.items():
            score = self.score_match(full_text, keywords)
            if score > 0:
                tech_scores[tech] = score

        if tech_scores:
            sorted_techs = sorted(tech_scores.items(), key=lambda x: x[1], reverse=True)
            for tech, _ in sorted_techs:
                if not any(self.is_similar_or_contains(tech, ex) for ex in excluded_tags):
                    tags.append(tech)
                if len(tags) >= 2:
                    break

        # 2. –ò—â–µ–º –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        if len(tags) < 3:
            concept_scores = {}
            for concept, keywords in DOMAIN_CONCEPTS.items():
                score = self.score_match(full_text, keywords)
                if score > 0:
                    concept_scores[concept] = score

            if concept_scores:
                sorted_concepts = sorted(concept_scores.items(), key=lambda x: x[1], reverse=True)
                for concept, _ in sorted_concepts:
                    if len(tags) >= 3:
                        break
                    if not any(self.is_similar_or_contains(concept, ex) for ex in excluded_tags + tags):
                        tags.append(concept)

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è
        if len(tags) < 3:
            noun_phrases = self.extract_meaningful_phrases(full_text)

            for phrase in noun_phrases:
                if len(tags) >= 3:
                    break
                if not any(self.is_similar_or_contains(phrase, ex) for ex in excluded_tags + tags):
                    tags.append(phrase)

        # 4. –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        if len(tags) < 1:
            normalized = self.normalize_text(full_text)

            if area and area in AREAS:
                area_words = set()
                for keyword in AREAS[area]["keywords"]:
                    area_words.update(keyword.lower().split())
                normalized = [w for w in normalized if w not in area_words]

            if normalized:
                counter = Counter(normalized)
                for word, _ in counter.most_common(3):
                    if not any(self.is_similar_or_contains(word, ex) for ex in excluded_tags + tags):
                        tags.append(word)
                    if len(tags) >= 3:
                        break

        # –§–∏–Ω–∞–ª—å–Ω—ã–π fallback
        if not tags:
            title_words = self.normalize_text(title)
            if title_words:
                tags = [title_words[0]]
            else:
                tags = ["—Ä–∞–∑–Ω–æ–µ"]

        return tags[:3]

    def determine_categories(self, title: str, description: str, area: str, thematic_tags: List[str]) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É–º–Ω—ã–º–∏ fallback'–∞–º–∏"""
        full_text = f"{title} {description}"

        excluded = [area] + thematic_tags

        scores = {}
        for category, keywords in CATEGORIES.items():
            scores[category] = self.score_match(full_text, keywords)

        sorted_categories = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        categories = []

        # –ë–µ—Ä—ë–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —Å–∫–æ—Ä–æ–º
        for cat, score in sorted_categories:
            if score > 0 and not any(self.is_similar_or_contains(cat, ex) for ex in excluded + categories):
                categories.append(cat)
            if len(categories) >= 3:
                break

        # Fallback 1: –∏—â–µ–º –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        if not categories:
            for concept, keywords in DOMAIN_CONCEPTS.items():
                if self.score_match(full_text, keywords) > 0:
                    if not any(self.is_similar_or_contains(concept, ex) for ex in excluded + categories):
                        categories.append(concept)
                    if len(categories) >= 3:
                        break

        # Fallback 2: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –æ–±–ª–∞—Å—Ç–∏
        if not categories and area in AREAS:
            default_cats = AREAS[area].get("default_categories", [])
            for cat in default_cats:
                if not any(self.is_similar_or_contains(cat, ex) for ex in excluded + categories):
                    categories.append(cat)
                if len(categories) >= 3:
                    break

        # Fallback 3: –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        if not categories:
            noun_phrases = self.extract_meaningful_phrases(description)
            for phrase in noun_phrases:
                if not any(self.is_similar_or_contains(phrase, ex) for ex in excluded + categories):
                    categories.append(phrase)
                if len(categories) >= 1:
                    break

        # Fallback 4: –±–µ—Ä—ë–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        if not categories:
            normalized = self.normalize_text(description)
            if normalized:
                counter = Counter(normalized)
                for word, _ in counter.most_common(3):
                    if not any(self.is_similar_or_contains(word, ex) for ex in excluded + categories):
                        categories.append(word)
                    if len(categories) >= 1:
                        break

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º thematic_tags –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if not categories and thematic_tags:
            categories = [thematic_tags[0]]

        return categories[:3]

    def determine_attributes(self, title: str, description: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –∫—É—Ä—Å–∞"""
        full_text = f"{title} {description}"

        attrs = []
        scores = {}

        for attr, keywords in ATTRIBUTES.items():
            score = self.score_match(full_text, keywords)
            if score > 0:
                scores[attr] = score

        if scores:
            attrs = [attr for attr, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)]

        if not attrs:
            practice_words = ["–¥–µ–ª–∞—Ç—å", "—Å–æ–∑–¥–∞—Ç—å", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å", "–ø—Ä–æ–µ–∫—Ç", "–ø—Ä–∏–º–µ–Ω–∏—Ç—å", "—Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å", "—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ",
                              "–∑–∞–¥–∞–Ω–∏–µ", "–∑–∞–¥–∞—á–∞"]
            if any(word in full_text.lower() for word in practice_words):
                attrs.append("–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π")
            elif any(word in full_text.lower() for word in ["—Ç–µ–æ—Ä–∏—è", "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–∏–∑—É—á–µ–Ω–∏–µ"]):
                attrs.append("—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π")
            else:
                if "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ" in full_text.lower() or "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ" in full_text.lower():
                    attrs.append("–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π")
                else:
                    attrs.append("—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π")

        return attrs

    def determine_difficulty(self, title: str, description: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        full_text = f"{title} {description}"

        scores = {}
        for level, keywords in DIFFICULTY.items():
            scores[level] = self.score_match(full_text, keywords)

        for level, indicators in DIFFICULTY_INDICATORS.items():
            indicator_score = self.score_match(full_text, indicators)
            scores[level] = scores.get(level, 0) + indicator_score * 0.5

        if scores and max(scores.values()) > 0:
            return max(scores, key=scores.get)

        full_lower = full_text.lower()

        if any(word in full_lower for word in ["–Ω–∞—á–∏–Ω", "–æ—Å–Ω–æ–≤", "–≤–≤–µ–¥–µ–Ω–∏–µ", "–±–∞–∑–æ–≤", "–ø–µ—Ä–≤—ã–π"]):
            return "–Ω–∞—á–∞–ª—å–Ω—ã–π"

        if any(word in full_lower for word in ["–ø—Ä–æ–¥–≤–∏–Ω", "—ç–∫—Å–ø–µ—Ä—Ç", "—Å–ª–æ–∂–Ω", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª"]):
            return "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π"

        return "—Å—Ä–µ–¥–Ω–∏–π"

    def generate_tags(self, title: str, description: str) -> Dict:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö —Ç–µ–≥–æ–≤ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"""
        area = self.determine_area(title, description)
        thematic_tags = self.determine_thematic_tags(title, description, area)
        categories = self.determine_categories(title, description, area, thematic_tags)
        attributes = self.determine_attributes(title, description)
        difficulty = self.determine_difficulty(title, description)

        return {
            "area": area,
            "thematic_tags": thematic_tags,
            "categories": categories,
            "attributes": attributes,
            "difficulty": difficulty
        }


def tag_courses_from_json(input_file: str, output_file: str):
    """
    –ß–∏—Ç–∞–µ—Ç –∫—É—Ä—Å—ã –∏–∑ JSON —Ñ–∞–π–ª–∞, —Ç–µ–≥–∏—Ä—É–µ—Ç –∏—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç

    Args:
        input_file: –ø—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É —Å –∫—É—Ä—Å–∞–º–∏
        output_file: –ø—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É —Å —Ç–µ–≥–∞–º–∏
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∫—É—Ä—Å–æ–≤ –∏–∑ {input_file}...")

    # –ß–∏—Ç–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    courses = data.get('courses', [])
    print(f"–ù–∞–π–¥–µ–Ω–æ –∫—É—Ä—Å–æ–≤: {len(courses)}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Ç–µ–≥–æ–≤...")
    generator = CourseTagGenerator()

    # –¢–µ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—É—Ä—Å
    tagged_courses = []
    for i, course in enumerate(courses, 1):
        course_id = course.get('id')
        name = course.get('name', '')
        description = course.get('description', '')

        print(f"\r–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—É—Ä—Å–∞ {i}/{len(courses)}: {name[:50]}...", end='')

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–≥–∏
        tags = generator.generate_tags(name, description)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏ –∫ –∫—É—Ä—Å—É
        tagged_course = {
            'id': course_id,
            'name': name,
            'description': description,
            'tags': tags
        }

        tagged_courses.append(tagged_course)

    print("\n\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_data = {
        'courses': tagged_courses,
        'total_courses': len(tagged_courses)
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "=" * 60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("=" * 60)

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –æ–±–ª–∞—Å—Ç–µ–π
    area_counter = Counter()
    difficulty_counter = Counter()

    for course in tagged_courses:
        area_counter[course['tags']['area']] += 1
        difficulty_counter[course['tags']['difficulty']] += 1

    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ–±–ª–∞—Å—Ç—è–º:")
    for area, count in area_counter.most_common():
        print(f"  {area}: {count} –∫—É—Ä—Å–æ–≤")

    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:")
    for difficulty, count in difficulty_counter.most_common():
        print(f"  {difficulty}: {count} –∫—É—Ä—Å–æ–≤")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    print("\n" + "=" * 60)
    print("–ü–†–ò–ú–ï–†–´ –¢–ï–ì–ò–†–û–í–ê–ù–ò–Ø (–ø–µ—Ä–≤—ã–µ 5 –∫—É—Ä—Å–æ–≤):")
    print("=" * 60)

    for course in tagged_courses[:5]:
        print(f"\nüìö {course['name']}")
        print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {course['description'][:80]}...")
        print(f"   –¢–µ–≥–∏:")
        for key, value in course['tags'].items():
            print(f"     {key}: {value}")


if __name__ == "__main__":
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    input_file = "courses.json"
    output_file = "courses_tagged.json"

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    tag_courses_from_json(input_file, output_file)